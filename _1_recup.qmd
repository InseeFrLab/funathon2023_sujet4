## 1. Récupération et nettoyage de la base `OpenFoodFacts`

### Préliminaire (🟡,🟢,🔵,🔴,⚫)

Comme nous allons utiliser fréquemment certains paramètres,
une bonne pratique consiste à les stocker dans un fichier
dédié, au format `YAML` et d'importer celui-ci via
`Python`. Ceci est expliqué dans [ce cours de l'ENSAE](https://ensae-reproductibilite.github.io/website/chapters/application.html#etape-3-gestion-des-param%C3%A8tres)

Nous proposons de créer le fichier suivant au nom `config.yaml`:

```yaml
URL_OPENFOOD: "https://static.openfoodfacts.org/data/en.openfoodfacts.org.products.csv.gz"
ENDPOINT_S3: "https://minio.lab.sspcloud.fr"
BUCKET: "projet-funathon"
DESTINATION_DATA_S3: "/2023/sujet4/diffusion"
URL_FASTTEXT_MINIO: "https://minio.lab.sspcloud.fr/projet-funathon/2023/sujet4/diffusion/model_coicop10.bin"
URL_COICOP_LABEL: "https://www.insee.fr/fr/statistiques/fichier/2402696/coicop2016_liste_n5.xls"
```

⚠️ Si vous désirez pouvoir reproduire tous les exemples de ce fichier, vous devez
changer la variable `BUCKET` pour mettre votre nom d'utilisateur sur le `SSPCloud`.

Nous allons lire ce fichier avec le package adapté pour transformer ces
instructions en variables `Python` (stockées dans un dictionnaire)



::: {.cell .markdown}
<!----- boite 🟢 ----->

```{=html}
```{python}
#| echo: false
#| output: asis
from utils_notebook import create_box_level
create_box_level(color = "🟢", title = "Utiliser un fichier YAML (🟢)")
```
<details>
<summary>Dérouler pour révéler les instructions</summary>
```

A partir des exemples présents dans [cette page](https://stackoverflow.com/questions/1773805/how-can-i-parse-a-yaml-file-in-python),
importer les variables dans un objet `Python` nommé `config`

```{=html}
</details>
</div>
```

<!----- end 🟢 ----->
:::

::: {.cell .markdown}
<!----- boite 🔵 ----->

```{=html}
```{python}
#| echo: false
#| output: asis
from utils_notebook import create_box_level
create_box_level(color = "🔵", title = "Utiliser un fichier YAML (🔵)")
```
<details>
<summary>Dérouler pour révéler les instructions</summary>
```

Utiliser le package `PyYAML` pour importer les éléments présents dans `config.yaml` dans un objet `Python` nommé `config`

```{=html}
</details>
</div>
```

<!----- end 🔵 ----->
:::

::: {.cell .markdown}
<!----- boite 🔴,⚫ ----->

```{=html}
```{python}
#| echo: false
#| output: asis
from utils_notebook import create_box_level
create_box_level(color = "🔴", title = "Utiliser un fichier YAML (🔴,⚫)")
```
<details>
<summary>Dérouler pour révéler les instructions</summary>
```

Importer les éléments présents dans `config.yaml` dans un objet `Python` nommé `config`

```{=html}
</details>
</div>
```

<!----- end 🔵 ----->
:::



```{python}
#| classes: yellow-code

# Solution pour voie 🟡
import yaml

def import_yaml(filename: str) -> dict:
    """
    Importer un fichier YAML

    Args:
        filename (str): Emplacement du fichier

    Returns:
        dict: Le fichier YAML sous forme de dictionnaire Python
    """
    with open(filename, "r", encoding="utf-8") as stream:
        config = yaml.safe_load(stream)
        return config

import_yaml("config.yaml")
```

Il est recommandé pour la suite de
copier-coller la fonction créée (ne pas oublier les imports associés) 
dans un fichier à l'emplacement `utils/import_yaml.py`. Cette approche modulaire est
une bonne
pratique, recommandée
dans [ce cours de l'ENSAE](https://ensae-reproductibilite.github.io/website/).

Pour la voie 🟡, ce fichier a déjà été créé pour vous. 
Le tester de la manière suivante:

```{python}
#| classes: yellow-code

# Solution pour voie 🟡
from utils.import_yaml import import_yaml
config = import_yaml("config.yaml")
```


### Télécharger et nettoyer la base `OpenFoodFacts` (🟡,🟢,🔵,🔴,⚫)

Un export quotidien de la
base de données `OpenFoodFacts` est fourni au format `CSV`. L'URL est le suivant:

```{python}
config["URL_OPENFOOD"]
```

Il est possible d'importer de plusieurs manières ce type de fichier avec `Python`. 
Ce qu'on propose ici, 
c'est de le faire en deux temps, afin d'avoir un contrôle des 
options mises en oeuvre lors de l'import (notamment le format de certaines variables) :

- Utiliser `requests` pour télécharger le fichier et l'écrire, de manière intermédiaire, 
sur le disque local ;
- Utiliser `pandas` avec quelques options pour importer le fichier puis le manipuler. 


::: {.cell .markdown}
<!----- boite 🟢 ----->

```{=html}
```{python}
#| echo: false
#| output: asis
from utils_notebook import create_box_level
create_box_level(color = "🟢", title = "Télécharger et importer OpenFoodFacts (🟢)")
```
<details>
<summary>Dérouler pour révéler les instructions</summary>
```

1. Utiliser la fonction `requests.get` pour télécharger le fichier.
Vous pouvez vous inspirer de réponses [ici](https://stackoverflow.com/questions/16694907/download-large-file-in-python-with-requests)


2. Utiliser `pd.read_csv` avec les options suivantes:
        + Le fichier utilise `\t` comme tabulation
        + Utiliser l'argument `parse_dates=["created_datetime", "last_modified_datetime", "last_image_datetime"]`
        + Il est nécessaire de figer quelques types avec l'argument `dtype`. Voici le dictionnaire à passer
        
```python
{
    "code ": "str",
    "emb_codes": "str",
    "emb_codes_tags": "str",
    "energy_100g": "float",
    "alcohol_100g": "float",
}
```

3. Forcer la colonne `code` à être de type _string_ avec la méthode `.astype(str)`

```{=html}
</details>
</div>
```

<!----- end 🟢 ----->
:::

::: {.cell .markdown}
<!----- boite 🔵 ----->

```{=html}
```{python}
#| echo: false
#| output: asis
from utils_notebook import create_box_level
create_box_level(color = "🔵", title = "Télécharger et importer OpenFoodFacts (🔵)")
```
<details>
<summary>Dérouler pour révéler les instructions</summary>
```

1. Utiliser le _package_ `requests` pour télécharger le fichier. Si vous voulez afficher une barre de progression,
vous pouvez vous inspirer de la fonction `download_pb` du package [`cartiflette`](https://github.com/InseeFrLab/cartiflette)

2. Lire les données avec `pandas` avec les options suivantes:
        + Le fichier utilise `\t` comme tabulation
        + Utiliser l'argument `parse_dates = ["created_datetime", "last_modified_datetime", "last_image_datetime"]`
        + Il est nécessaire de figer, voici le dictionnaire à passer
        
```python
{
    "code ": "str",
    "emb_codes": "str",
    "emb_codes_tags": "str",
    "energy_100g": "float",
    "alcohol_100g": "float",
}
```

3. Forcer la colonne `code` à être de type _string_

```{=html}
</details>
</div>
```

<!----- end 🔵 ----->
:::

::: {.cell .markdown}
<!----- boite 🔴,⚫ ----->

```{=html}
```{python}
#| echo: false
#| output: asis
from utils_notebook import create_box_level
create_box_level(color = "grey", title = "Télécharger et importer OpenFoodFacts (🔴,⚫)")
```
<details>
<summary>Dérouler pour révéler les instructions</summary>
```

1. Télécharger le fichier avec `Python`. Pour s'assurer de la progression du téléchargement, 
utiliser également la librairie `tqdm`.

2. Lire les données avec `pandas` avec les options suivantes:
        + Le fichier utilise `\t` comme tabulation
        + Utiliser l'argument `parse_dates = ["created_datetime", "last_modified_datetime", "last_image_datetime"]`
        + Il est nécessaire de figer, voici le dictionnaire à passer
        
```python
{
    "code ": "str",
    "emb_codes": "str",
    "emb_codes_tags": "str",
    "energy_100g": "float",
    "alcohol_100g": "float",
}
```

3. Forcer la colonne `code` à être de type _string_

```{=html}
</details>
</div>
```

<!----- end 🔴,⚫ ----->
:::

```{python}
#| classes: yellow-code
#| label: import-openfood-solution
#| eval: false

# Solution pour voie 🟡
from utils.preprocess_openfood import download_openfood, import_openfood
download_openfood(destination = "openfood.csv.gz")
openfood = import_openfood("openfood.csv.gz")
openfood.loc[:, ['code', 'product_name', 'energy-kcal_100g', 'nutriscore_grade']].sample(5, random_state = 12345)
```

```{python}
#| label: import-openfood
#| echo: false
#| output: false
#| cache: true
import os
import pandas as pd

from utils.preprocess_openfood import download_openfood, import_openfood
download_openfood(destination = "openfood.csv.gz")
if os.path.exists("openfood.parquet"):
    openfood = pd.read_parquet("openfood.parquet")
else:
    openfood = import_openfood("openfood.csv.gz")
    openfood.to_parquet("openfood.parquet")
```

```{python}
#| echo: false
#| output: true
openfood.loc[:, ['code', 'product_name', 'energy-kcal_100g', 'nutriscore_grade']].sample(5, random_state = 12345)
```

### Classification automatique dans une nomenclature de produits  (🟡,🟢,🔵,🔴,⚫)

Pour proposer sur notre application quelques statistiques pertinentes sur
le produit, nous allons associer chaque ligne d'`OpenFoodFacts` 
à un type de produit dans la `COICOP` pour pouvoir comparer un produit
à des produits similaires. 

Nous allons ainsi utiliser le nom du produit pour inférer le type de bien
dont il s'agit. Pour cela, dans les parcours 🟡,🟢 et 🔵, 
nous allons d'utiliser un classifieur expérimental
proposé sur [`Github InseeFrLab/predicat`](https://github.com/InseeFrLab/predicat)
qui a été entrainé sur cette tâche sur un grand volume de
données (non spécifiquement alimentaires). Pour les parcours 🔴 et ⚫, nous 
proposons d'entraîner un classifieur en utilisant la catégorisation des données
disponible directement dans `OpenFoodFacts`. Il est proposé d'utiliser `Fasttext`
(une librairie spécialisée open-source, développée par `Meta` il y a quelques années) dans
le cadre de la voie 🔴. Les personnes suivant la voie ⚫ sont libres d'utiliser
n'importe quel _framework_ de classification. 


::: {.cell .markdown}
<!----- boite 🟢 ----->

```{=html}
```{python}
#| echo: false
#| output: asis
from utils_notebook import create_box_level
create_box_level(color = "🟢", title = "Nettoyer les données textuelles (🟢)")
```
<details>
<summary>Dérouler pour révéler les instructions</summary>
```

1. Récupérer le dictionnaire de règles dans [ce fichier](https://raw.githubusercontent.com/InseeFrLab/predicat/master/app/utils_ddc.py)
2. Créer une colonne `preprocessed_labels` en appliquant la méthode `str.upper` à la colonne `product_name` afin de la mettre en majuscule
3. Modifier le `DataFrame` avec la syntaxe prenant la forme `data.replace({variable: dict_rules_replacement}, regex=True)`
4. Observer les cas où il y a eu des changements, par exemple de la manière suivante

```python
openfood.loc[:, ["product_name", "preprocessed_labels"]].dropna().loc[openfood["product_name"].str.upper() != openfood["preprocessed_labels"]]
```

```{=html}
</details>
</div>
```

<!----- end 🟢 ----->
:::


::: {.cell .markdown}
<!----- boite 🔵 ----->

```{=html}
```{python}
#| echo: false
#| output: asis
from utils_notebook import create_box_level
create_box_level(color = "🔵", title = "Nettoyer les données textuelles (🔵)")
```
<details>
<summary>Dérouler pour révéler les instructions</summary>
```

1. Récupérer le dictionnaire de règles dans [ce fichier](https://raw.githubusercontent.com/InseeFrLab/predicat/master/app/utils_ddc.py)
2. Créer une colonne `preprocessed_labels` mettant en majuscule la colonne `product_name`
3. Modifier le `DataFrame` avec la syntaxe utilisant la méthode `replace` (celle qui s'applique aux `DataFrame`, pas celle s'appliquant à une `Serie`) et le dictionnaire adapté
4. Observer les cas où il y a eu des changements,

```{=html}
</details>
</div>
```

<!----- end 🔵 ----->
:::

::: {.cell .markdown}
<!----- boite 🔴 ----->

```{=html}
```{python}
#| echo: false
#| output: asis
from utils_notebook import create_box_level
create_box_level(color = "🔴", title = "Nettoyer les données textuelles (🔴)")
```
<details>
<summary>Dérouler pour révéler les instructions</summary>
```

1. Récupérer le dictionnaire de règles dans [ce fichier](https://raw.githubusercontent.com/InseeFrLab/predicat/master/app/utils_ddc.py)
2. Créer une colonne `preprocessed_labels` appliquant les remplacements à `product_name` grâce à la méthode `replace` (celle qui s'applique aux `DataFrame`, pas celle s'appliquant à une `Serie`)
3. Observer les cas où il y a eu des changements

```{=html}
</details>
</div>
```

<!----- end 🔴 ----->
:::

::: {.cell .markdown}
<!----- boite ⚫ ----->

```{=html}
```{python}
#| echo: false
#| output: asis
from utils_notebook import create_box_level
create_box_level(color = "⚫", title = "Nettoyer les données textuelles (⚫)")
```
<details>
<summary>Dérouler pour révéler les instructions</summary>
```

1. Récupérer le dictionnaire de règles dans [ce fichier](https://raw.githubusercontent.com/InseeFrLab/predicat/master/app/utils_ddc.py)
2. Créer une colonne `preprocessed_labels` appliquant les remplacements à `product_name`
3. Observer les cas où il y a eu des changements

```{=html}
</details>
</div>
```

<!----- end ⚫ ----->
:::


Dans un premier temps, on récupère les fonctions permettant d'appliquer sur nos données 
le même _preprocessing_ que celui qui a été mis en oeuvre lors de l'entraînement du modèle:

```{python}
#| classes: yellow-code
#| label: get-utils-ddc
#| output: false

# Solution pour voie 🟡 et 🟢
from utils.download_pb import download_pb
download_pb("https://raw.githubusercontent.com/InseeFrLab/predicat/master/app/utils_ddc.py", "utils/utils_ddc.py")
```

Pour observer les nettoyages de champs textuels mis en oeuvre, les lignes suivantes
peuvent être exécutées:

```{python}
#| output: false

from utils.utils_ddc import replace_values_ean
replace_values_ean
```

Pour effectuer des remplacements dans des champs textuels, le plus simple est d'utiliser
les expressions régulières (`regex`). Vous pouvez trouver une ressource complète
sur le sujet dans [ce cours de `Python` de l'ENSAE](https://pythonds.linogaliana.fr/regex/).

Deux options s'offrent à nous:

- Utiliser le _package_ `re` et boucler sur les lignes
- Utiliser les fonctionnalités très pratiques de `Pandas`

Nous privilégierons la deuxième approche, plus naturelle quand on utilise des `DataFrames` et
plus efficace puisqu'elle est nativement intégrée à `Pandas`. 

La syntaxe prend la forme suivante : 

```python
data.replace({variable: dict_rules_replacement}, regex=True)
```

C'est celle qui est implémentée dans la fonction _ad hoc_ du script `utils/preprocess_openfood.py`.
Cette dernière s'utilise de la manière suivante:

```{python}
from utils.utils_ddc import replace_values_ean
from utils.preprocess_openfood import clean_column_dataset
openfood = clean_column_dataset(openfood,replace_values_ean, "product_name", "preprocessed_labels")
```

Voici quelques cas où notre nettoyage de données a modifié le nom du produit :

```{python}
openfood.loc[:, ["product_name", "preprocessed_labels"]].dropna().loc[openfood["product_name"].str.upper() != openfood["preprocessed_labels"]]
```

On peut remarquer que pour aller plus loin et améliorer la normalisation des champs,
il serait pertinent d'appliquer un certain nombre de nettoyages supplémentaires, comme
le retrait des mots de liaison (_stop words_). Des exemples de ce type de nettoyages
sont présents dans le [cours de `Python` de l'ENSAE](https://pythonds.linogaliana.fr/nlpintro/).
Cela est laissé comme exercice aux voies 🔴 et ⚫


::: {.cell .markdown}
<!----- boite 🔴,⚫ ----->

```{=html}
```{python}
#| echo: false
#| output: asis
from utils_notebook import create_box_level
create_box_level(color = "grey", title = "Normaliser les champs textuels (🔴,⚫)")
```
<details>
<summary>Dérouler pour révéler les instructions</summary>
```

Utiliser `nltk` ou `SpaCy` (solution préférable) pour ajouter des nettoyages
de champs textuels

```{=html}
</details>
</div>
```

<!----- end 🔴,⚫ ----->
:::

On peut maintenant se tourner vers la classification à proprement parler. 
Pour celle-ci, on propose d'utiliser un modèle qui a été entrainé
avec [`Fasttext`](https://fasttext.cc/). Voici comment récupérer le modèle
et le tester sur un exemple très basique:

```{python}
from utils.download_pb import download_pb
import os
import fasttext

if os.path.exists("fasttext_coicop.bin") is False:
    download_pb(url = config["URL_FASTTEXT_MINIO"], fname = "fasttext_coicop.bin")


model = fasttext.load_model("fasttext_coicop.bin")
model.predict("RATATOUILLE")
```

Le résultat est peu intelligible. En effet, cela demande une bonne connaissance de la 
COICOP pour savoir de
manière intuitive que cela correspond à la catégorie [_"Autres plats cuisinés à base de légumes"_](https://www.insee.fr/fr/statistiques/serie/001764476). 

Avant de généraliser le classifieur à l'ensemble de nos données, on se propose donc de récupérer
les noms des COICOP depuis le site [insee.fr](https://www.insee.fr/fr/metadonnees/coicop2016/division/01?champRecherche=true).
Comme cela ne présente pas de défi majeur, le code est directement proposé, quelle que soit la voie empruntée:

```{python}
def import_coicop_labels(url: str) -> pd.DataFrame:
    coicop = pd.read_excel(url, skiprows=1)
    coicop['Code'] = coicop['Code'].str.replace("'", "")
    coicop = coicop.rename({"Libellé": "category"}, axis = "columns")
    return coicop
    
coicop = import_coicop_labels(
    "https://www.insee.fr/fr/statistiques/fichier/2402696/coicop2016_liste_n5.xls"
)
coicop.loc[coicop["Code"].str.contains("01.1.7.3.2")]
```

Maintenant nous avons tous les ingrédients pour généraliser notre recette `fasttext`.
L'application en série de prédictions via `fasttext` étant un peu fastidieuse et peu élégante (elle
nécessite d'être à l'aise avec les lists `Python`) et n'étant pas le centre de notre sujet,
la fonction suivante est fournie pour effectuer cette opération :

```{python}
def model_predict_coicop(data, model, product_column: str = "preprocessed_labels", output_column: str = "coicop"):
    predictions = pd.DataFrame(
        {
        output_column: \
            [k[0] for k in model.predict(
                [str(libel) for libel in data[product_column]], k = 1
                )[0]]
        })

    data[output_column] = predictions[output_column].str.replace(r'__label__', '')
    return data

openfood = model_predict_coicop(openfood, model)
```

### Version alternative via l'API [`predicat`](https://github.com/InseeFrLab/predicat)  (🟡,🟢,🔵,🔴,⚫)


L'utilisation d'API pour accéder à des données devient de plus en plus fréquente. Mais
les API peuvent servir à faire beaucoup plus, notamment à récupérer des prédictions
d'un modèle. La plateforme [`HuggingFace`](https://huggingface.co/) est très appréciée
pour cela: elle a grandement facilité la réutilisation de modèles mis en disposition
en _open source_. Cette approche a principalement deux avantages:

- Elle permet d'appliquer sur les données fournies en entrée exactement les mêmes pré-traitement
que sur les données d'entrainement. Ceci renforce la fiabilité des prédictions. 
- Elle facilite le travail des data scientists ou statisticiens car ils ne sont plus obligés 
de mettre en place des fonctions compliquées pour passer les prédictions dans une colonne
de `DataFrame`. 

Ici, nous proposons de tester une API mise à disposition
de manière expérimentale pour faciliter la réutilisation de notre modèle de classification. 
Cette API s'appelle `predicat` et son code source est
disponible sur [`Github`](https://github.com/InseeFrLab/predicat).
Pour les parcours 🟡,🟢,🔵, nous suggérons de se cantonner à tester quelques exemples. 
Pour les parcours 🔴 et ⚫ qui voudraient se tester sur les API,
nous proposons de généraliser ces appels à [`predicat`](https://github.com/InseeFrLab/predicat)
pour classifier toutes nos données. 

::: {.cell .markdown}
<!----- boite 🔴,⚫ ----->

```{=html}
```{python}
#| echo: false
#| output: asis
from utils_notebook import create_box_level
create_box_level(color = "grey", title = "Consommer un modèle sous forme d'API (🔴,⚫)")
```
<details>
<summary>Dérouler pour révéler les instructions</summary>
```

Appliquer l'API [`predicat`](https://github.com/InseeFrLab/predicat) en série pour
catégoriser l'ensemble des données

```{=html}
</details>
</div>
```

<!----- end 🔴,⚫ ----->
:::


Voici, pour les parcours 🟡,🟢,🔵, un exemple d'utilisation:

```{python}
import requests

def predict_from_api(product_name):
    url_api = f"https://api.lab.sspcloud.fr/predicat/label?k=1&q=%27{product_name}%27"
    output_api_predicat = requests.get(url_api).json()
    coicop_found = output_api_predicat['coicop'][f"'{product_name}'"][0]['label']
    return coicop_found

predict_from_api("Ratatouille")
```

Pour le parcours 🔵, voici un exercice pour tester sur un échantillon des données
de l'`OpenFoodFacts`


```{=html}
```{python}
#| echo: false
#| output: asis
from utils_notebook import create_box_level
create_box_level(color = "🔵", title = "Consommer un modèle sous forme d'API (🔵)")
```
<details>
<summary>Dérouler pour révéler les instructions</summary>
```

A partir des exemples présents dans [ce _notebook_](https://github.com/InseeFrLab/predicat/blob/master/help/example-request.ipynb),
tester l'API sur une centaine de noms de produits pris aléatoirement (ceux avant _preprocessing_).


```{=html}
</details>
</div>
```

<!----- end 🔵 ----->
:::


### Ecriture de la base sur l'espace de stockage distant

Le fait d'avoir effectué en amont ce type d'opération permettra
d'économiser du temps par la suite puisqu'on s'évite des calculs à la
volée coûteux en performance (rien de pire qu'une page _web_ qui rame non ?). 

Pour facilement retrouver ces données, on va les écrire dans un espace
de stockage accessible facilement. Pour cela, nous proposons d'utiliser celui
du SSP Cloud. 


